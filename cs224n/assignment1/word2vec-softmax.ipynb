{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax for word2vec\n",
    "Compute cost, gradients for softmax in word2vec model\n",
    "* parameters: predicted, outputVectors, target\n",
    "\n",
    "\\begin{align}\n",
    "\\hat y_{o} =p(o|c) & = \\frac {e^{u_{o}^\\top v_{c}}} {\\sum_{w=1}^{V} e^{u_{w}^\\top v_{c}}} \\\\\n",
    "J_{softmax-CE}(o, v_{c}, U) & = CE(y_{o},\\hat y_{o}) = -y_{o} \\log \\hat y_{o} \\\\\n",
    "& = -\\log e^{u_{o}^\\top v_{c}} + \\log \\sum_{w=1}^{V} e^{u_{w}^\\top v_{c}} \\\\\n",
    "& = -u_{o}^\\top v_{c}  + \\log \\sum_{w=1}^{V} e^{u_{w}^\\top v_{c}} \\\\\n",
    "\\frac {\\partial{J}} {\\partial{v_{c}}} & = -u_{o} + \\sum_{w=1}^{V} \\hat y_{w} u_{w} \\\\\n",
    "\\frac {\\partial{J}} {\\partial{u_{o}}} & = (\\hat y_{w} -1) v_{c} \\\\\n",
    "\\frac {\\partial{J}} {\\partial{u_{w}}} & = \\hat y_{w} v_{c}, \\quad \\text {for all}\\; w \\neq o\n",
    "\\end{align}\n",
    "\n",
    "In the following code, **predicted** is vector $v_{c}$, **outputVectors** is matrix $U$, **target** is subscript $o$, **cost** is $J_{softmax-CE}(o, v_{c}, U)$, **gradPred** is $\\frac {\\partial{J}} {\\partial{v_{c}}}$, and **grad** are $\\frac {\\partial{J}} {\\partial{u_{o}}}$ and $\\frac {\\partial{J}} {\\partial{u_{w}}}$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array([0.26726124, 0.53452248, 0.80178373])\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.reshape(predicted, (-1, 1))\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputVectors = np.array([\n",
    "       [0.26726124, 0.53452248, 0.80178373],\n",
    "       [0.45584231, 0.56980288, 0.68376346],\n",
    "       [0.50257071, 0.57436653, 0.64616234],\n",
    "       [0.52342392, 0.57576631, 0.62810871]])\n",
    "outputVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.dot(outputVectors, predicted)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_exp = np.sum(np.exp(output))\n",
    "sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = (np.log(sum_exp) - output[target])[0]\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats = np.exp(output) / sum_exp\n",
    "y_hats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats[target] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradPred = np.dot(outputVectors.T, y_hats)\n",
    "gradPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.dot(y_hats, predicted.T)\n",
    "grad.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
