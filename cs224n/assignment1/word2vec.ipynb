{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model\n",
    "\n",
    "Representing words by their context\n",
    "* Core idea: A word's meaning is given by the words that frequently appear close-by\n",
    "* When a word $w$ appears in a text, its context is the set of words that apear nearby (within a fixed-size window).\n",
    "* Use the many contexts of $w$ to build up a representation of $w$\n",
    "\n",
    "Word vectors\n",
    "\n",
    "We will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. Word vectors are sometimes called **word embeddings** or **word representations**.\n",
    "\n",
    "Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Idea:\n",
    "\n",
    "* We have a large corpus of text\n",
    "* Every word in a fixed vocabulary is represented by avector\n",
    "* Go through each position $t$ in the text, which has a center word $c$ and context(\"outside\") words $o$\n",
    "* Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)\n",
    "* Keep adjusting the word vectors to maximize this probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective functions\n",
    "For each position t = 1, ...,T, predict context words within a window of fixed size m, given center word w.\n",
    "\n",
    "Likelikood\n",
    "\n",
    "$$L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{t+j}|w_{t};\\theta)$$\n",
    "\n",
    "Negative log likelihood\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log { P(w_{t+j}|w_{t};\\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate $P(w_{t+j}|w{t};\\theta)$?\n",
    "\n",
    "We will use two vectors per word $w$:\n",
    "* $v_{w}$ when w is a center word\n",
    "* $u_{w}$ whe w is a context word\n",
    "\n",
    "Then for a center word $c$ and a context word $o$:\n",
    "\n",
    "$$P(o|c) = \\frac{exp(u_{o}^\\top v_{c})}{\\sum_{w \\in V} exp(u_{w}^\\top v_{c})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs for sampling methods\n",
    "The costs of a single context word in different sampling methods\n",
    "\n",
    "* $J_{softmax-CE}(o, v_{c}, U) = -u_{o}^\\top v_{c}  + \\log \\sum_{w=1}^{V} e^{u_{w}^\\top v_{c}}$\n",
    "* $J_{neg-sample}(o, v_{c}, U) = -\\log(\\delta(u_{o}^\\top v_{c}) - \\sum_{k=1}^{K} \\log(\\delta(-u_{k}^\\top v_{c}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two model variants\n",
    "* Skip-gram(SG): Predict context(\"outside\") words (position independent) given center word\n",
    "* Continuous Bag of Words(CBOW): Predict center word from (bag of) context words\n",
    "\n",
    "We can unify the cost formula on the above as $J(o,\\hat {v},U)$, where\n",
    "* $\\hat{v} = v_{c}$ in SG\n",
    "* $\\hat{v} = \\sum_{-m \\leq j \\leq m, j \\neq 0} v_{w_{t+j}} $ in CBOW\n",
    "\n",
    "So, the cost for a context centered around c are\n",
    "* $J_{skip-gram}(w_{t-m}\\dots w_{t+m}) = \\sum_{-m \\leq j \\leq m, j \\neq 0} J(w_{t+j},\\hat {v},U)$\n",
    "* $J_{CBOW}(w_{t-m}\\dots w_{t+m}) = J(w_{t},\\hat {v},U)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "Gradients for Skip-gram model\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {J_{skip-gram}(w_{t-m}\\dots w_{t+m})}}{\\partial {U}} & = \\sum_{-m \\leq j \\leq m, j \\neq 0} \\frac{\\partial {J(w_{t+j},\\hat {v},U)}}{\\partial {U}} \\\\\n",
    "\\frac{\\partial {J_{skip-gram}(w_{t-m}\\dots w_{t+m})}}{\\partial {v_c}} & = \\sum_{-m \\leq j \\leq m, j \\neq 0} \\frac{\\partial {J(w_{t+j},\\hat {v},U)}}{\\partial {v_c}} \\\\\n",
    "\\frac{\\partial {J_{skip-gram}(w_{t-m}\\dots w_{t+m})}}{\\partial {v_{w_{t+j}}}} & = 0, \\text{for all}\\; j \\neq c\n",
    "\\end{align}\n",
    "\n",
    "Gradients for CBOW model\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {J_{CBOW}(w_{t-m}\\dots w_{t+m})}}{\\partial {U}} & = \\frac{\\partial {J(w_{t},\\hat {v},U)}}{\\partial {U}} \\\\\n",
    "\\frac{\\partial {J_{CBOW}(w_{t-m}\\dots w_{t+m})}}{\\partial {v_{w_{t+j}}}} & = \\frac{\\partial {J(w_{t},\\hat {v},U)}}{\\partial {\\hat {v}}}, \\text{for all}\\;j \\in \\{-m,\\dots,-1,+1,\\dots,+m\\} \\\\\n",
    "\\frac{\\partial {J_{CBOW}(w_{t-m}\\dots w_{t+m})}}{\\partial {v_{w_{t+j}}}} & = 0, \\text{for all}\\;j \\not\\in \\{-m,\\dots,-1,+1,\\dots,+m\\} \\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
